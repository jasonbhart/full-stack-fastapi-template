# ============================================================================
# Full Stack FastAPI Template - Environment Configuration Example
# ============================================================================
# This file documents all available environment variables for the template.
# Copy this file to .env and customize values for your environment.
#
# Environment-specific configuration:
# - .env.local.example: Local development settings template (copy to .env.local)
# - .env.production: Production deployment settings template
# - .env: Base configuration (loaded first, then overlaid by environment-specific file)
#
# Setup for local development:
# 1. Copy .env.local.example to .env.local
# 2. Set APP_ENV=local in .env (or use default)
# 3. Configure your API keys in .env.local
# ============================================================================

# ============================================================================
# GENERAL CONFIGURATION
# ============================================================================

# Domain - Used by Traefik to transmit traffic and acquire TLS certificates
# For local development: localhost
# For testing Traefik config locally: localhost.tiangolo.com
# For production: your-domain.com
DOMAIN=localhost

# Frontend URL - Used by the backend to generate links in emails
# Development: http://localhost:5173
# Production: https://dashboard.your-domain.com
FRONTEND_HOST=http://localhost:5173

# Environment - Legacy environment indicator (local, staging, production)
# NOTE: APP_ENV is now preferred for environment-specific configuration
ENVIRONMENT=local

# Application Environment - Controls environment-specific .env file loading
# Values: local, staging, production
# When set, loads .env.{APP_ENV} to override base .env settings
APP_ENV=local

# Project Name - Displayed in emails and documentation
PROJECT_NAME="Full Stack FastAPI Project"

# Stack Name - Used for Docker Compose stack identification
STACK_NAME=full-stack-fastapi-project

# ============================================================================
# BACKEND CONFIGURATION
# ============================================================================

# CORS Origins - Comma-separated list of allowed origins
# Include all frontend URLs that will access the API
BACKEND_CORS_ORIGINS="http://localhost,http://localhost:5173,https://localhost,https://localhost:5173"

# Secret Key - Used for JWT token signing and other cryptographic operations
# IMPORTANT: Generate a secure random string for production
# Example: openssl rand -hex 32
SECRET_KEY=changethisnowplease

# First Superuser - Initial admin account created on startup
FIRST_SUPERUSER=admin@example.com
FIRST_SUPERUSER_PASSWORD=changethisnowplease

# ============================================================================
# EMAIL CONFIGURATION
# ============================================================================

# SMTP Server Configuration
SMTP_HOST=
SMTP_USER=
SMTP_PASSWORD=
SMTP_PORT=587
SMTP_TLS=True
SMTP_SSL=False

# Email Sender Information
EMAILS_FROM_EMAIL=info@example.com

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================

# PostgreSQL Database Connection
POSTGRES_SERVER=localhost
POSTGRES_PORT=5432
POSTGRES_DB=app
POSTGRES_USER=app
POSTGRES_PASSWORD=appappappapp

# ============================================================================
# LLM & AGENT CONFIGURATION
# ============================================================================

# LangChain Configuration
# LangSmith API key for tracing and monitoring (optional)
LANGCHAIN_API_KEY=

# Enable LangChain tracing (true/false)
# Set to true to send traces to LangSmith
LANGCHAIN_TRACING_V2=false

# LangSmith project name for organizing traces
LANGCHAIN_PROJECT=

# LangChain API endpoint (usually no need to change)
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com

# LLM Model Configuration
# Primary model to use for agent operations
# Examples: gpt-4, gpt-4-turbo, gpt-3.5-turbo, claude-3-sonnet-20240229
LLM_MODEL_NAME=gpt-4

# Model provider: openai, anthropic, or azure
LLM_MODEL_PROVIDER=openai

# Model temperature (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.7

# Maximum tokens per response
LLM_MAX_TOKENS=2048

# Provider API Keys
# OpenAI API key for GPT models
OPENAI_API_KEY=

# Anthropic API key for Claude models
ANTHROPIC_API_KEY=

# Azure OpenAI Configuration (if using Azure)
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_API_VERSION=2024-02-15-preview

# ============================================================================
# OBSERVABILITY & TRACING
# ============================================================================

# Langfuse Configuration - LLM observability and analytics platform
# Get API keys from https://cloud.langfuse.com or your self-hosted instance

# Langfuse API keys
LANGFUSE_SECRET_KEY=
LANGFUSE_PUBLIC_KEY=

# Langfuse host URL
# Cloud: https://cloud.langfuse.com
# Self-hosted: https://your-langfuse-instance.com
LANGFUSE_HOST=https://cloud.langfuse.com

# Enable/disable Langfuse tracing (true/false)
LANGFUSE_ENABLED=false

# Sampling rate for traces (0.0 to 1.0)
# 1.0 = trace all requests, 0.5 = trace 50% of requests
LANGFUSE_SAMPLE_RATE=1.0

# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================

# Evaluation API Configuration
# API key for the LLM used to evaluate agent outputs
# Can be the same as OPENAI_API_KEY or a separate key for cost control
EVALUATION_API_KEY=

# Base URL for evaluation LLM API
# OpenAI: https://api.openai.com/v1
# Azure: https://your-resource.openai.azure.com
EVALUATION_BASE_URL=https://api.openai.com/v1

# Model to use for evaluations (typically a cost-effective model)
# Recommended: gpt-4o-mini, gpt-3.5-turbo
# This model judges agent outputs using LLM-as-judge pattern
EVALUATION_LLM=gpt-4o-mini

# Sleep time between evaluations (seconds)
# Helps avoid rate limiting when evaluating many traces
# Increase if you hit rate limits
EVALUATION_SLEEP_TIME=1

# ============================================================================
# RATE LIMITING
# ============================================================================

# Enable/disable rate limiting (true/false)
RATE_LIMIT_ENABLED=true

# Rate limits for agent endpoints
# Requests per minute per user
RATE_LIMIT_PER_MINUTE=60

# Redis Configuration (used for rate limiting)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0

# ============================================================================
# MONITORING & LOGGING
# ============================================================================

# Sentry DSN - Error tracking and performance monitoring
# Get DSN from https://sentry.io
SENTRY_DSN=

# Structured Logging Configuration
# Log format and level are controlled by APP_ENV:
# - local: DEBUG level, colored console output with correlation/trace IDs
# - staging: INFO level, colored console output with correlation/trace IDs
# - production: WARNING level, JSON-formatted output for log aggregation
#
# All logs include:
# - correlation_id: Request-level correlation ID (from X-Correlation-ID header or auto-generated)
# - trace_id: Langfuse trace ID when available (links logs to LLM traces)
# - timestamp, level, logger, module, function, line number
# - environment context (APP_ENV value)
#
# Correlation IDs are automatically:
# - Generated for each request (or extracted from X-Correlation-ID header)
# - Added to response headers (X-Correlation-ID, X-Trace-ID)
# - Included in all log messages during request processing
# - Linked to Langfuse traces when agent operations are performed

# ============================================================================
# FRONTEND FEATURE FLAGS
# ============================================================================

# Agent/AI Assistant Feature
# Enable/disable the AI agent chat interface and related functionality
# When enabled, shows agent chat UI, history, and navigation menu items
# When disabled, agent features are completely hidden from the UI
# Values: true, false, 1, 0 (case-insensitive)
# Default: false
VITE_ENABLE_AGENT=false

# ============================================================================
# INFRASTRUCTURE SERVICES
# ============================================================================

# Langfuse Database Password (separate PostgreSQL instance for Langfuse)
LANGFUSE_DB_PASSWORD=langfuse

# Grafana Admin Credentials
# WARNING: The example defaults below are ONLY safe for local development
# For staging/production: Override via environment variables or CI/CD secrets
# Generate a secure password: openssl rand -hex 32
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=admin

# Traefik Admin Authentication
# Used to protect Traefik dashboard and Prometheus endpoints via HTTP Basic Auth
# Required when using docker-compose.traefik.yml for production deployments
#
# USERNAME: Admin username for Basic Auth
# HASHED_PASSWORD: Bcrypt-hashed password (MUST be single-quoted to prevent shell expansion)
#
# To generate a hashed password:
# docker run --rm httpd:2.4-alpine htpasswd -nbB admin "your-password" | cut -d ":" -f 2
# OR using Apache utils:
# htpasswd -nbB admin "your-password" | cut -d ":" -f 2
#
# Example for local development (username: admin, password: admin):
# USERNAME=admin
# HASHED_PASSWORD='$2y$05$kGZ6vH3zXqKXqJqKgj4XqOZq5TkB0fBXKzD9C9TkXqDfDfDfDfDfG'
#
# WARNING: Change these values for production! Default values are insecure.
# IMPORTANT: Always wrap HASHED_PASSWORD in single quotes to prevent $ expansion
USERNAME=admin
HASHED_PASSWORD=

# ============================================================================
# DOCKER CONFIGURATION
# ============================================================================

# Docker registry images for deployment
# Change these to your Docker registry paths for production
DOCKER_IMAGE_BACKEND=backend
DOCKER_IMAGE_FRONTEND=frontend
